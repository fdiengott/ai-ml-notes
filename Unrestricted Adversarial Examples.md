#alignment-technique 
An example of a [[Black-box attack]]

Examples designed to make a model misbehave, created by another language model, where the model can generate any examples. 

'Restricted' adversarial examples are usually closely related to training datapoints. 
