A phenomenon in [[Interpretability]] where a "pure neuron" (a neural that is excited by one thing) in one layer, is then connected with other unrelated features in a later layer. 

The thinking behind this phenomenon is that it is a way for a network to conserve neurons by linking together two things that don't (generally) co-occur. 

Superposition is also described by a model representing more features than it has dimensions