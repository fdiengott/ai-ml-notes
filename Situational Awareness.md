Large models currently have a great amount of factual knowledge about the world, but don't yet reliably apply it when choosing actions. 

*Situational awareness* is a skill where a model identifies which abstract knowledge is relevant to its policies or context in which its policies are acting and uses that knowledge to choose actions. 

## Situationally-aware [[Reward Hacking]]
A situationally aware model might be able to reason about flaws in its own feedback mechanisms to train it. 