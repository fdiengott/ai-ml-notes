Even if an agent is trained on a "correct" reward function, it might still learn goals which generalize in undesirable ways

## Promising responses
- [[Mechanistic interpretability]]
- [[Recursive evaluation]]
