A piece of the alignment problem with the goal being to precisely specify the purpose of a system

> “An objective function r is outer aligned if all models that perform optimally on r in the limit of perfect training and infinite data are intent aligned.”
> - [“Risks from Learned Optimization” paper](https://www.greaterwrong.com/s/r9tYkB2a8Fp4DN8yB) by Hubinger et al.

